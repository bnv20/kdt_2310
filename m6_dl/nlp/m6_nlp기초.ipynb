{"cells":[{"cell_type":"markdown","metadata":{"id":"LfgikJTPIuND"},"source":["### 텍스트의 토큰화\n","- NLP(자연어 처리) 토큰화는 텍스트 분석, 언어 번역, 감정 분석 등과 같은 NLP 작업을 위해 텍스트를 준비하는 프로세스의 기본 단계.\n","- 토큰화에는 텍스트를 단어, 문구, 기호 또는 토큰이라고 하는 기타 의미 있는 요소와 같은 개별 구성 요소로 분해하는 작업이 포함. 토큰은 추가 처리 및 분석을 위한 구성 요소가 된다.\n","- 토큰화의 목적은 텍스트 데이터를 단순화하고 알고리즘이 이해하고 처리할 수 있도록 관리하기 쉽게 만드는 것이다. NLP 작업의 특정 요구 사항에 따라 토큰은 단어, 문장 또는 단어의 일부(예: 어간 또는 하위 단어)일 수도 있다. 이 프로세스는 불필요한 구두점과 공백을 제거하는 데 도움이 되며 데이터 세트 전체에서 일관성을 유지하기 위해 모든 텍스트를 소문자로 변환하는 작업도 포함될 수 있다."]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9Eirt2_1IuNE","executionInfo":{"status":"ok","timestamp":1706968776160,"user_tz":-540,"elapsed":423,"user":{"displayName":"kevin park","userId":"02703084888761299921"}},"outputId":"4394ff7e-c83f-4ff9-83a3-59e1f42e59f1"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","원문:\n"," 해보지 않으면 해낼 수 없다\n","\n","토큰화:\n"," ['해보지', '않으면', '해낼', '수', '없다']\n"]}],"source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense,Flatten,Embedding\n","from tensorflow.keras.utils import to_categorical\n","from numpy import array\n","\n","# 케라스의 텍스트 전처리와 관련한 함수중 text_to_word_sequence 함수를 불러옵니다.\n","from tensorflow.keras.preprocessing.text import text_to_word_sequence\n","\n","# 전처리할 텍스트를 정합니다.\n","text = '해보지 않으면 해낼 수 없다'\n","\n","# 해당 텍스트를 토큰화합니다.\n","result = text_to_word_sequence(text)\n","print(\"\\n원문:\\n\", text)\n","print(\"\\n토큰화:\\n\", result)"]},{"cell_type":"markdown","source":["- Tokenizer' 클래스는 텍스트를 정수 시퀀스로 변환하도록 설계\n","- fit_on_texts(docs): 이 메소드는 문장 목록(docs)을 인수로 사용하여 token 개체에서 호출된다. 텍스트 목록을 기반으로 내부 어휘를 업데이트하여 토크나이저가 이러한 텍스트로 작업할 수 있도록 준비한다. 말뭉치의 각 고유 단어에 색인을 할당하고 단어 빈도와 같은 다양한 측정항목을 계산하는 작업이 포함된다.\n","- token.word_counts: 토크나이저를 텍스트에 맞춘 후 word_counts는 키가 입력 텍스트에서 발견된 단어이고 값은 각 단어의 발생 횟수인 OrderedDict를 제공한다. 'OrderedDict'를 사용하면 단어가 텍스트에서 처음 나타나는 순서대로 정렬.\n","- token.document_count: 이 속성은 처리된 총 문서(또는 문장) 수를 표시\n","- token.word_docs: word_counts와 유사한 OrderedDict이지만 단어의 빈도 대신 각 단어가 나타나는 문서 수를 표시\n","- token.word_index: 이 속성은 단어를 고유하게 할당된 정수에 매핑하는 OrderedDict를 제공. 모델에는 숫자 입력이 필요하므로 이는 기계 학습 모델의 텍스트를 벡터화하는 데 필요\n"],"metadata":{"id":"MYQnLKugMPdI"}},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XLgxsUDJIuNG","executionInfo":{"status":"ok","timestamp":1706969476676,"user_tz":-540,"elapsed":588,"user":{"displayName":"kevin park","userId":"02703084888761299921"}},"outputId":"574212c9-ef5f-4655-fa40-5b648dac227b"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","단어 카운트:\n"," OrderedDict([('먼저', 1), ('텍스트의', 2), ('각', 1), ('단어를', 1), ('나누어', 1), ('토큰화', 1), ('합니다', 1), ('단어로', 1), ('토큰화해야', 1), ('딥러닝에서', 2), ('인식됩니다', 1), ('토큰화한', 1), ('결과는', 1), ('사용할', 1), ('수', 1), ('있습니다', 1)])\n","\n","문장 카운트:  3\n","\n","각 단어가 몇 개의 문장에 포함되어 있는가:\n"," defaultdict(<class 'int'>, {'합니다': 1, '각': 1, '텍스트의': 2, '먼저': 1, '토큰화': 1, '나누어': 1, '단어를': 1, '딥러닝에서': 2, '인식됩니다': 1, '토큰화해야': 1, '단어로': 1, '수': 1, '토큰화한': 1, '사용할': 1, '결과는': 1, '있습니다': 1})\n","\n","각 단어에 매겨진 인덱스 값:\n"," {'텍스트의': 1, '딥러닝에서': 2, '먼저': 3, '각': 4, '단어를': 5, '나누어': 6, '토큰화': 7, '합니다': 8, '단어로': 9, '토큰화해야': 10, '인식됩니다': 11, '토큰화한': 12, '결과는': 13, '사용할': 14, '수': 15, '있습니다': 16}\n"]}],"source":["# 단어 빈도수 세기\n","\n","# 전처리하려는 세 개의 문장을 정합니다.\n","docs = ['먼저 텍스트의 각 단어를 나누어 토큰화 합니다.',\n","       '텍스트의 단어로 토큰화해야 딥러닝에서 인식됩니다.',\n","       '토큰화한 결과는 딥러닝에서 사용할 수 있습니다.',\n","       ]\n","\n","# 토큰화 함수를 이용해 전처리 하는 과정입니다.\n","token = Tokenizer()             # 토큰화 함수 지정\n","token.fit_on_texts(docs)       # 토큰화 함수에 문장 적용\n","\n","# 단어의 빈도수를 계산한 결과를 각 옵션에 맞추어 출력합니다.\n","# Tokenizer()의 word_counts 함수는 순서를 기억하는 OrderedDict 클래스를 사용합니다.\n","print(\"\\n단어 카운트:\\n\", token.word_counts)\n","print(\"\\n문장 카운트: \", token.document_count)\n","print(\"\\n각 단어가 몇 개의 문장에 포함되어 있는가:\\n\", token.word_docs)\n","# token.word_index의 출력 순서는 제공된 텍스트 코퍼스의 각 단어의 빈도에 따라 가장 빈번한 단어부터 가장 빈도가 낮은 단어까지 결정\n","print(\"\\n각 단어에 매겨진 인덱스 값:\\n\",  token.word_index)"]},{"cell_type":"markdown","metadata":{"id":"NqekQ9beIuNH"},"source":["### 단어의 원-핫 인코딩\n","- word_size = len(token.word_index) + 1: 여기서 token.word_index는 키가 단어이고 값이 해당 고유 정수 인덱스인 사전. token.word_index의 길이는 말뭉치에 있는 고유 단어의 총 개수를 나타낸다. 이 개수에 1을 추가하는 것은 \"0\" 인덱스를 설명하기 위해 NLP에서 일반적인 관행이며는 종종 패딩에 사용되거나 구현에 따라 알 수 없는 단어를 나타낼 수 있습니다.\n","- x = to_categorical(x, num_classes=word_size): 이 줄은 단어를 나타내는 정수 인덱스의 목록 또는 배열이어야 하는 x를 원-핫 인코딩 형식으로 변환한다. to_categorical은 클래스 벡터(정수)를 바이너리 클래스 행렬로 변환하는 함수(일반적으로 Keras의)이며 x의 각 정수에 대해 to_categorical은 1로 설정된 정수 인덱스에 해당하는 위치를 제외하고 모든 요소가 0인 길이 word_size의 벡터를 생성한다.\n","- num_classes=word_size는 총 클래스 수를 지정한다. 이 경우 어휘 크기(word_size)로 설정되어 원-핫 인코딩에 어휘의 모든 단어에 대한 슬롯과 추가 \"0\" 인덱스가 있는지 확인한다.\n"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NkTULnMMIuNH","executionInfo":{"status":"ok","timestamp":1706969852749,"user_tz":-540,"elapsed":582,"user":{"displayName":"kevin park","userId":"02703084888761299921"}},"outputId":"c13b4023-d902-41cb-cb0d-71613fc665a4"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'오랫동안': 1, '꿈꾸는': 2, '이는': 3, '그': 4, '꿈을': 5, '닮아간다': 6}\n"]}],"source":["text=\"오랫동안 꿈꾸는 이는 그 꿈을 닮아간다\"\n","token = Tokenizer()\n","token.fit_on_texts([text])\n","print(token.word_index)"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S4RoTv6oIuNI","executionInfo":{"status":"ok","timestamp":1706969857034,"user_tz":-540,"elapsed":586,"user":{"displayName":"kevin park","userId":"02703084888761299921"}},"outputId":"3efc7b13-b091-4e06-eacd-428d1c064457"},"outputs":[{"output_type":"stream","name":"stdout","text":["[[1, 2, 3, 4, 5, 6]]\n"]}],"source":["x=token.texts_to_sequences([text])\n","print(x)"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZBOp32XWIuNI","executionInfo":{"status":"ok","timestamp":1706969861551,"user_tz":-540,"elapsed":13,"user":{"displayName":"kevin park","userId":"02703084888761299921"}},"outputId":"d64a63e0-91e1-4b90-e50a-0e66a74ab74e"},"outputs":[{"output_type":"stream","name":"stdout","text":["[[[0. 1. 0. 0. 0. 0. 0.]\n","  [0. 0. 1. 0. 0. 0. 0.]\n","  [0. 0. 0. 1. 0. 0. 0.]\n","  [0. 0. 0. 0. 1. 0. 0.]\n","  [0. 0. 0. 0. 0. 1. 0.]\n","  [0. 0. 0. 0. 0. 0. 1.]]]\n"]}],"source":["#인덱스 수에 하나를 추가해서 원-핫 인코딩 배열 만들기\n","word_size = len(token.word_index) + 1\n","x = to_categorical(x, num_classes=word_size)\n","print(x)"]},{"cell_type":"markdown","metadata":{"id":"85oIzCxUIuNI"},"source":["###.텍스트를 읽고 긍정, 부정 예측하기"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ou6_T8srIuNJ","executionInfo":{"status":"ok","timestamp":1706969886318,"user_tz":-540,"elapsed":739,"user":{"displayName":"kevin park","userId":"02703084888761299921"}},"outputId":"17c7275b-90ee-4415-a255-17c28e2d0aeb"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'너무': 1, '재밌네요': 2, '최고예요': 3, '참': 4, '잘': 5, '만든': 6, '영화예요': 7, '추천하고': 8, '싶은': 9, '영화입니다': 10, '한번': 11, '더': 12, '보고싶네요': 13, '글쎄요': 14, '별로예요': 15, '생각보다': 16, '지루하네요': 17, '연기가': 18, '어색해요': 19, '재미없어요': 20}\n"]}],"source":["# 텍스트 리뷰 자료를 지정합니다.\n","docs = [\"너무 재밌네요\",\"최고예요\",\"참 잘 만든 영화예요\",\"추천하고 싶은 영화입니다\",\"한번 더 보고싶네요\",\"글쎄요\",\"별로예요\",\"생각보다 지루하네요\",\"연기가 어색해요\",\"재미없어요\"]\n","\n","# 긍정 리뷰는 1, 부정 리뷰는 0으로 클래스를 지정합니다.\n","classes = array([1,1,1,1,1,0,0,0,0,0])\n","\n","# 토큰화\n","token = Tokenizer()\n","token.fit_on_texts(docs)\n","print(token.word_index)\n"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p1PYghSoIuNJ","executionInfo":{"status":"ok","timestamp":1706969904345,"user_tz":-540,"elapsed":424,"user":{"displayName":"kevin park","userId":"02703084888761299921"}},"outputId":"15e5e1c0-180c-42ba-b10c-44b7ad5db83c"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","리뷰 텍스트, 토큰화 결과:\n"," [[1, 2], [3], [4, 5, 6, 7], [8, 9, 10], [11, 12, 13], [14], [15], [16, 17], [18, 19], [20]]\n"]}],"source":["x = token.texts_to_sequences(docs)\n","print(\"\\n리뷰 텍스트, 토큰화 결과:\\n\",  x)"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GyqWw39VIuNJ","executionInfo":{"status":"ok","timestamp":1706969912156,"user_tz":-540,"elapsed":418,"user":{"displayName":"kevin park","userId":"02703084888761299921"}},"outputId":"bd1f04e2-4008-4b9c-c350-11f9d3d0d1f0"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","패딩 결과:\n"," [[ 0  0  1  2]\n"," [ 0  0  0  3]\n"," [ 4  5  6  7]\n"," [ 0  8  9 10]\n"," [ 0 11 12 13]\n"," [ 0  0  0 14]\n"," [ 0  0  0 15]\n"," [ 0  0 16 17]\n"," [ 0  0 18 19]\n"," [ 0  0  0 20]]\n"]}],"source":["# 패딩, 서로 다른 길이의 데이터를 4로 맞추어 줍니다.\n","padded_x = pad_sequences(x, 4)\n","print(\"\\n패딩 결과:\\n\", padded_x)"]},{"cell_type":"markdown","source":["### Embedding\n","- 기계 학습, 특히 자연어 처리(NLP)의 맥락에서 임베딩은 단어, 구문 또는 기타 유형의 엔터티를 실수의 밀집된 벡터로 표현하는 기술을 의미. 핵심 아이디어는 이러한 엔터티의 의미론적 의미를 연속적인 벡터 공간으로 인코딩하는 것이다. 여기서 벡터 간의 기하학적 관계는 이들이 나타내는 엔터티 간의 의미론적 관계를 반영하며 이 접근 방식은 의미론적 관계가 캡처되지 않는 원-핫 인코딩과 같은 희소 표현과 대조된다.\n","- 의미:\n","  - 의미적 표현: 임베딩 벡터는 단어나 개체의 의미를 포착하도록 설계된다. 비슷한 의미를 가진 단어는 임베딩 공간에서 서로 가까운 벡터를 갖도록 처리된다.\n","  - 차원성 감소: 임베딩은 고차원 공간(예: 원-핫 인코딩된 벡터)의 단어를 저차원의 연속 벡터 공간으로 매핑하며 이는 표현을 더욱 효율적으로 만들고 계산 복잡성을 줄인다.\n","  - 컨텍스트화: 고급 임베딩 모델(예: Word2Vec, GloVe, BERT)에서는 단어가 나타나는 컨텍스트가 벡터 표현에 영향을 미치므로 모델이 다양한 컨텍스트에서 단어의 다양한 의미를 캡처할 수 있다.\n","- 임베딩 생성 방법:\n","  - 사전 훈련된 임베딩: 일반적인 접근 방식 중 하나는 대규모 텍스트 모음에 대해 사전 훈련된 임베딩을 사용하는 것으로 Word2Vec, GloVe 및 FastText와 같은 모델을 사용하면 훈련 코퍼스에서 풍부한 의미 체계 관계를 학습한 사전 훈련된 모델을 기반으로 단어를 벡터에 매핑할 수 있다.\n","    - Word2Vec: 컨텍스트를 사용하여 대상 단어를 예측하거나(CBOW) 단어를 사용하여 컨텍스트를 예측(Skip-gram)하여 임베딩을 학습한다.\n","    - GloVe: 단어 동시 발생 통계 행렬을 인수분해하여 임베딩을 학습한다.\n","    - FastText: Word2Vec과 유사하지만 접두사와 접미사의 의미를 포착하는 하위 단어 정보를 포함한다.\n","  - 자신만의 임베딩 훈련: 신경망을 사용하여 처음부터 자신만의 임베딩을 훈련할 수도 있다."],"metadata":{"id":"p4AA1vsaXMxm"}},{"cell_type":"markdown","source":["텍스트 입력을 표현하기 위해 단어 임베딩 활용\n","- model.add(Embedding(word_size, 8, input_length=4)): 이 줄은 모델에 임베딩 레이어를 추가. 임베딩 레이어는 단어 임베딩을 만드는 데 사용. 각 매개변수의 의미는 다음과 같다.\n","  - word_size: 입력 차원의 크기, 즉 어휘 크기. 데이터 세트에 있는 총 고유 단어 수에 1을 더한 값.\n","  - 8: 임베딩 벡터의 크기. 각 단어는 8차원 벡터로 표현.\n","  input_length=4: 입력 시퀀스의 길이. 이 모델은 각 입력 시퀀스의 길이가\n","  - 4일 것으로 예상(예: 입력당 4개의 단어).\n","- model.add(Flatten()): 임베딩 레이어 이후 출력 모양은 배치 크기를 포함하여 3차원. 이 레이어는 출력을 2차원(배치 크기, input_length * 8)으로 평면화하여 밀도가 높은 레이어에 직접 공급될 수 있도록 한다."],"metadata":{"id":"WY1JmrvjVrVD"}},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GtOhm3pmIuNK","executionInfo":{"status":"ok","timestamp":1706969944067,"user_tz":-540,"elapsed":609,"user":{"displayName":"kevin park","userId":"02703084888761299921"}},"outputId":"813d046a-d585-405b-913f-d5d656f88907"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_1 (Embedding)     (None, 4, 8)              168       \n","                                                                 \n"," flatten_1 (Flatten)         (None, 32)                0         \n","                                                                 \n"," dense_1 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 201 (804.00 Byte)\n","Trainable params: 201 (804.00 Byte)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}],"source":["# 임베딩에 입력될 단어의 수를 지정합니다.\n","word_size = len(token.word_index) +1\n","\n","# 단어 임베딩을 포함하여 딥러닝 모델을 만들고 결과를 출력합니다.\n","model = Sequential()\n","model.add(Embedding(word_size, 8, input_length=4))\n","model.add(Flatten())\n","model.add(Dense(1, activation='sigmoid'))\n","model.summary()"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6xilXjEtIuNK","executionInfo":{"status":"ok","timestamp":1706970259644,"user_tz":-540,"elapsed":1653,"user":{"displayName":"kevin park","userId":"02703084888761299921"}},"outputId":"8f04fdb0-d847-4a2d-f74a-29a246b74992"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","1/1 [==============================] - 1s 718ms/step - loss: 0.6984 - accuracy: 0.4000\n","Epoch 2/20\n","1/1 [==============================] - 0s 9ms/step - loss: 0.6961 - accuracy: 0.4000\n","Epoch 3/20\n","1/1 [==============================] - 0s 8ms/step - loss: 0.6938 - accuracy: 0.5000\n","Epoch 4/20\n","1/1 [==============================] - 0s 8ms/step - loss: 0.6915 - accuracy: 0.7000\n","Epoch 5/20\n","1/1 [==============================] - 0s 8ms/step - loss: 0.6892 - accuracy: 0.7000\n","Epoch 6/20\n","1/1 [==============================] - 0s 8ms/step - loss: 0.6869 - accuracy: 0.7000\n","Epoch 7/20\n","1/1 [==============================] - 0s 8ms/step - loss: 0.6846 - accuracy: 0.8000\n","Epoch 8/20\n","1/1 [==============================] - 0s 7ms/step - loss: 0.6823 - accuracy: 0.9000\n","Epoch 9/20\n","1/1 [==============================] - 0s 8ms/step - loss: 0.6800 - accuracy: 0.9000\n","Epoch 10/20\n","1/1 [==============================] - 0s 8ms/step - loss: 0.6777 - accuracy: 0.9000\n","Epoch 11/20\n","1/1 [==============================] - 0s 7ms/step - loss: 0.6754 - accuracy: 0.9000\n","Epoch 12/20\n","1/1 [==============================] - 0s 8ms/step - loss: 0.6731 - accuracy: 1.0000\n","Epoch 13/20\n","1/1 [==============================] - 0s 7ms/step - loss: 0.6708 - accuracy: 1.0000\n","Epoch 14/20\n","1/1 [==============================] - 0s 7ms/step - loss: 0.6685 - accuracy: 1.0000\n","Epoch 15/20\n","1/1 [==============================] - 0s 7ms/step - loss: 0.6662 - accuracy: 0.9000\n","Epoch 16/20\n","1/1 [==============================] - 0s 7ms/step - loss: 0.6638 - accuracy: 0.9000\n","Epoch 17/20\n","1/1 [==============================] - 0s 8ms/step - loss: 0.6615 - accuracy: 0.9000\n","Epoch 18/20\n","1/1 [==============================] - 0s 8ms/step - loss: 0.6592 - accuracy: 0.9000\n","Epoch 19/20\n","1/1 [==============================] - 0s 8ms/step - loss: 0.6568 - accuracy: 0.9000\n","Epoch 20/20\n","1/1 [==============================] - 0s 9ms/step - loss: 0.6545 - accuracy: 0.9000\n","1/1 [==============================] - 0s 131ms/step - loss: 0.6522 - accuracy: 0.9000\n","\n"," Accuracy: 0.9000\n"]}],"source":["model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","model.fit(padded_x, classes, epochs=20)\n","print(\"\\n Accuracy: %.4f\" % (model.evaluate(padded_x, classes)[1]))"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"V100"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}